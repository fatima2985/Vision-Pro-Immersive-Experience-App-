Immersence is an interactive mixed reality experience for Apple Vision Pro, where users create and experience sound through movement. It transforms physical gestures into a dynamic blend of visuals and audio, making users feel like they are painting with music in a 360-degree shared space.

Core Features
Movement-Driven Sound: Users’ motions generate different sounds—soft chimes, deep beats, flowing melodies—mapped to specific gestures.
Reactive Visuals: Colors and abstract forms dynamically shift based on movement, speed, and intensity.
Expressive Interaction: Faster motions create bright, energetic visuals and sharp tones, while slower gestures produce smoother, calming effects.
Multi-User Expansion (Future Update): Later versions will allow multiple users to interact in the same space, blending their sound and visuals into a collaborative, evolving composition.
Inspiration & Previous Work
This builds upon previous experiments using the human body as an instrument, now expanded into an immersive, 360-degree mixed reality experience.

Previous Work:

Human Body as an Instrument https://www.youtube.com/watch?v=NlOuqKbCXEI
Interactive Sound Visuals https://www.youtube.com/watch?v=k-58hPrwwU4
Visual & Environmental Inspiration:

Abstract Generative Visuals https://www.youtube.com/watch?v=4XeSs9a1rLQ
Immersive 3D Spaces https://www.youtube.com/watch?v=x1EVhNM-uf4
Flowing Audio-Reactive Forms https://www.youtube.com/watch?v=9BH-8W_6N8A

Development Roadmap
Prototype Single-User Experience
Implement motion tracking
Develop real-time visual/audio mapping
Optimize for immersive interaction
Enhance Visual & Sound Interaction
Expand the range of sounds and visual responses
Improve fluidity and responsiveness
Multi-User Expansion
Enable shared soundscapes for multiple participants
Synchronize visuals across users for collaborative creation
Next Steps
Develop an app to bring the experience to Vision Pro.
Build dynamic visuals that react to user actions.
Integrate sound mapping to connect movement with musical expression.
